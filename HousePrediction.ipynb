{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Price Prediction\n",
    "\n",
    "- [1 - Introduction](#Introduction)\n",
    "    - [1.1 - Project Overview](#Project-Overview)\n",
    "    - [1.2 - Problem Statement](#Problem-Statement)\n",
    "    - [1.3 - Dataset Description](#Dataset-Description)\n",
    "\n",
    "- [2 - Import Libraries](#Import-Libraries)\n",
    "\n",
    "- [3 - Data Loading and Exploration](#Data-Loading-and-Exploration)\n",
    "    - [3.1 - Load the Dataset](#Load-the-Dataset)\n",
    "    - [3.2 - Display Basic Information](#Display-Basic-Information)\n",
    "    \n",
    "- [4 - Data Preprocessing](#Data-Preprocessing)\n",
    "    - [4.1 - Removing Irrelevant Features](#Removing-Irrelevant-Features)\n",
    "    - [4.2 - Handle Missing Values](#Handle-Missing-Values)\n",
    "    - [4.3 - Encoding Categorical Variables](#Encoding-Categorical-Variables)\n",
    "    - [4.4 - Feature Engineering](#Feature-Engineering)\n",
    "    - [4.5 - Outlier Removal](#Outlier-Removal)\n",
    "    - [4.6 - Further Encoding Categorical Variables](#Futher-Encoding-Categorical-Variables)\n",
    "    - [4.7 - Feature Scaling](#Feature-Scaling)\n",
    "\n",
    "\n",
    "- [5 - Data Splitting](#Data-Splitting)\n",
    "    - [5.1 - Split into Train, Validation, and Test Sets](#Split-into-Train-Validation-and-Test-Sets)\n",
    "    - [5.2 - Split Data into Features (X) and Target (y)](#Split-Data-into-Features-X-and-Target-y)\n",
    "\n",
    "\n",
    "- [6 - Model Definition and Training](#Model-Definition)\n",
    "    - [6.1 - Defining Models](#Defining-Models)\n",
    "    - [6.2 - Finding Best Model](#Finding-Best-Model)\n",
    "    - [6.3 - Fine-Tuning Model](#Fine-Tuning-Model)\n",
    "\n",
    "- [7 - Model Evaluation](#Model-Evaluation)\n",
    "\n",
    "- [8 - Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Introduction\n",
    "\n",
    "## 1.1 - Project Overview\n",
    "The goal of this project is to develop a predictive model that can estimate the prices of houses in Bengaluru, India. Accurately predicting house prices is crucial for real estate agents, buyers, and sellers to make informed decisions. By analyzing various factors such as the size of the property, location, and available amenities, we aim to build a machine learning model that can effectively predict house prices based on historical data.\n",
    "\n",
    "## 1.2 - Problem Statement\n",
    "The real estate market in Bengaluru is dynamic and influenced by multiple factors, making it challenging to estimate property prices accurately. The primary objective of this project is to address the following questions:\n",
    "\n",
    "- Can we build an accurate model to predict house prices using historical real estate data from Bengaluru?\n",
    "- How can we interpret the model's predictions to provide actionable insights for real estate professionals and potential buyers?\n",
    "\n",
    "By answering these questions, we aim to create a tool that can assist in making more accurate and informed real estate decisions.\n",
    "\n",
    "## 1.3 - Dataset Description\n",
    "The dataset used in this project is sourced from Kaggle and contains detailed information on various properties in Bengaluru, India.\n",
    "\n",
    "### Bengaluru House Data\n",
    "Each row in the dataset represents a property listing, and each column provides different attributes about the properties.\n",
    "\n",
    "- **Number of Rows:** 13,320 (properties)\n",
    "- **Number of Columns:** 9 (features)\n",
    "- **Target Column:** \"price\"\n",
    "\n",
    "### Data Composition\n",
    "The dataset includes the following information:\n",
    "\n",
    "- **Area Type:**\n",
    "  - The type of area (e.g., Super built-up Area, Plot Area, Built-up Area).\n",
    "\n",
    "- **Availability:**\n",
    "  - The availability status of the property (e.g., Ready to Move, available from a specific date).\n",
    "\n",
    "- **Location:**\n",
    "  - The location of the property within Bengaluru.\n",
    "\n",
    "- **Size:**\n",
    "  - The size of the property in terms of the number of bedrooms (e.g., 2 BHK, 3 Bedroom).\n",
    "\n",
    "- **Total Area:**\n",
    "  - The total area of the property in square feet.\n",
    "\n",
    "- **Number of Bathrooms:**\n",
    "  - The number of bathrooms available in the property.\n",
    "\n",
    "- **Number of Balconies:**\n",
    "  - The number of balconies available in the property.\n",
    "\n",
    "This dataset provides a comprehensive view of the real estate market in Bengaluru, allowing us to analyze and model the factors that influence house prices effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [2 - Import Libraries](#Import-Libraries)\n",
    "\n",
    "In this section, we import the necessary libraries required for data manipulation, visualization, and building a machine learning model using Sklearn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries for data manipulation and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn for data preprocessing, building, training the model and evaluation\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [3 - Data Loading and Exploration](#Data-Loading-and-Exploration)\n",
    "\n",
    "## [3.1 - Load the Dataset](#Load-the-Dataset)\n",
    "\n",
    "In this section, we will load the Bengaluru House dataset into a pandas DataFrame for further exploration and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into a pandas DataFrame\n",
    "data_path = './Bengaluru_House_Data.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows of the dataset to verify loading\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3.2 - Display Basic Information](#Display-Basic-Information)\n",
    "\n",
    "In this section, we will display basic information about the dataset to understand its structure and contents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the basic information about the dataset\n",
    "df.info()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [4 - Data Preprocessing](#Data-Preprocessing)\n",
    "\n",
    "## [4.1 - Removing Irrelevant Features](#Removing-Irrelevant-Features)\n",
    "\n",
    "In this section we will remove irrelevant features which we assume do not have any decisive weight for the target (house price)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will remove the 'availability' feature from the dataframe, as it is considered irrelevant for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove irrelevant features from the dataframe\n",
    "df.drop(['availability'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4.2 - Handle Missing Values](#Handle-Missing-Values)\n",
    "\n",
    "In this section, we will identify and handle missing values in the dataset to ensure the data is clean and ready for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5502 out of 13320 Samples do not have a value for **society**, therefore we will drop society too as feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['society'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will clean all Samples which do not have a value for the balcony feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['balcony'])\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets drop the row which do not have a value for the location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['location'])\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4.3 - Encoding Categorical Variables](#Encoding-Categorical-Variables)\n",
    "\n",
    "In this section, we handle the categorical variables present in the dataset by converting them into a numerical format that can be used by our machine learning model. We use **One-Hot Encoding** to achieve this, which transforms each categorical variable into a set of binary columns (0 or 1), representing the presence or absence of each category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making size numerical feature\n",
    "df['size'] = df['size'].apply(lambda x: float(x.split(' ')[0]))\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Total_Sqft** feature is not numerical. Lets find out how the input of these feature look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a method for detecting whether a object is a float or not\n",
    "def isFloat(x):\n",
    "    try:\n",
    "        float(x)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "# Filtering the Total_Sqft column for containing NON Float Values\n",
    "df[~df['total_sqft'].apply(isFloat)].head(40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the total_sqft column contains strings describing the Square in form like:\n",
    "2100 - 2850, 1005.03 - 1252.49\n",
    "\n",
    "The following code will convert this type of strings into floats.\n",
    "Square Foot Strings with other forms we will drop to make it easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_float(x):\n",
    "    try:\n",
    "        # Case 1: Range values, e.g., \"2100 - 2850\"\n",
    "        if '-' in x:\n",
    "            parts = x.split('-')\n",
    "            return (float(parts[0].strip()) + float(parts[1].strip())) / 2\n",
    "        \n",
    "        # Default case: Single float value, e.g., \"2100\"\n",
    "        else:\n",
    "            return float(x.strip())\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Applying the function to the total_sqft column\n",
    "df['total_sqft']= df['total_sqft'].apply(convert_to_float)\n",
    "\n",
    "# For Edge Cases where the conversion failed delete the rows\n",
    "df = df.dropna(subset=['total_sqft'])\n",
    "\n",
    "print(\"NaN Values per Column\")\n",
    "print(df.isnull().sum())\n",
    "df.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4.4 - Feature Engineering](#Feature-Engineering)\n",
    "\n",
    "In this section, we will introduce a new feature that **will assist in identifying and removing outliers** in the dataset. By engineering this additional feature, we aim to capture more nuanced patterns in the data that may not be immediately apparent from the existing features. This new feature will provide valuable insights for subsequent steps, particularly during the outlier detection and removal process, ultimately contributing to a more robust and accurate predictive model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new feature 'square_meter_price'\n",
    "df['square_meter_price'] = df['price'] * 100000 / df['total_sqft']\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4.5 - Outlier Removal](#Outlier-Removal)\n",
    "\n",
    "In this section, we will identify and remove outliers using the newly engineered feature. Removing these anomalies ensures a cleaner dataset and improves the model's accuracy and reliability.\n",
    "\n",
    "In this step we will remove real estates that are too extreme based on their properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all real estate properties with square_meter / bedrooms less than 300\n",
    "print(df.square_meter_price.describe())\n",
    "df_tmp = df[(df['total_sqft'] / df['size'] < 300)]\n",
    "print(df_tmp[['total_sqft', 'size', 'bath','balcony']].head())\n",
    "\n",
    "df = df[~(df['total_sqft'] / df['size'] < 300)]\n",
    "df.square_meter_price.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function removes outliers through the price_per_sqft column for each location in the dataset. It keeps only the data points within one standard deviation of the mean for each location, effectively filtering out extreme values that could skew the analysis or model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.square_meter_price.describe()\n",
    "\n",
    "def remove_pps_outliers(df):\n",
    "    df_out = pd.DataFrame()\n",
    "    for key, subdf in df.groupby('location'):\n",
    "        m = np.mean(subdf.square_meter_price)\n",
    "        st = np.std(subdf.square_meter_price)\n",
    "        reduced_df = subdf[(subdf.square_meter_price>(m-st)) & (subdf.square_meter_price<=(m+st))]\n",
    "        df_out = pd.concat([df_out,reduced_df],ignore_index=True)\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers from the dataset using the method defined above\n",
    "df_new = remove_pps_outliers(df)\n",
    "\n",
    "print(\"Removed Samples: \", df.shape[0] - df_new.shape[0])\n",
    "\n",
    "df = df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4.6 - Further Encoding Categorical Variables](#Futher-Encoding-Categorical-Variables)\n",
    "The next step is to convert the **location** feature to a numerical feature by using One Hot Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing Amount of Unique Values in the Location Column\n",
    "print(len(df.location.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount of Locations with less than 10 entries\n",
    "print(len(df['location'].value_counts()[df['location'].value_counts() < 10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the dataset and enhance the effectiveness of model training, we will replace all locations that appear fewer than 10 times with the label 'Other'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Get the locations with fewer than 10 occurrences\n",
    "rare_locations = df['location'].value_counts()[df['location'].value_counts() < 10].index\n",
    "\n",
    "# Replace rare locations with 'Other'\n",
    "df.loc[df['location'].isin(rare_locations), 'location'] = 'Other'\n",
    "\n",
    "df.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's encode the location feature by using One Hot Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the categorical feature\n",
    "df = pd.get_dummies(df, columns=['location'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to convert the **area_type** feature to a numerical feature by using One Hot Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many unique values are there in the area_type column\n",
    "print(df['area_type'].nunique())\n",
    "\n",
    "# Print the names of the unique area types\n",
    "print(df['area_type'].unique())\n",
    "\n",
    "\n",
    "#Encoding the categorical feature\n",
    "df = pd.get_dummies(df, columns=['area_type'])\n",
    "\n",
    "# Print the number of columns in the dataframe after encoding \n",
    "print(len(df.columns))\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4.7 - Feature Scaling](#Feature-Scaling)\n",
    "\n",
    "In this section, we will apply feature scaling and normalization to ensure that all features contribute equally to the model, as algorithms like linear regression can be sensitive to the scale of input data. This step is crucial to improve the model's performance by preventing features with larger ranges from dominating the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Normalize the size, bath, and balcony columns\n",
    "df[['size', 'bath', 'balcony']] = scaler.fit_transform(df[['size', 'bath', 'balcony']])\n",
    "\n",
    "# Standardize the total_sqft, price, and square_meter_price columns\n",
    "df[['total_sqft', 'price', 'square_meter_price']] = scaler.fit_transform(df[['total_sqft', 'price', 'square_meter_price']])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [5 - Data Splitting](#Data-Splitting)\n",
    "\n",
    "## [5.1 Split into Train and Test Set](#Split-into-Train-and-Test-Sets)\n",
    "\n",
    "\n",
    "In this chapter, we will split the dataset into training, and testing sets. \n",
    "This step is essential to evaluate the model's performance, tune hyperparameters, and ensure its generalizability to unseen data.\n",
    "We will use K-fold cross-validation afterwards so we won't need a validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, split the data into training and test sets (80% train, 20% test)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the sizes of each set to verify the split\n",
    "print(\"Training set size:\", len(train_df))\n",
    "print(\"Test set size:\", len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5.2 - Split Data into Features (X) and Target (y)](#Split-Data-into-Features-X-and-Target-y)\n",
    "\n",
    "In this section, we will divide our dataset into two main components: Features (X) and the target variable (y). The features (X) consist of all the independent variables that will be used as input to the model, while the target variable (y) represents the outcome we aim to predict—in this case, customer churn. This separation is crucial for training and evaluating the model effectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target column\n",
    "target_column = 'price'\n",
    "\n",
    "# Split the training set into features (X_train) and target (y_train)\n",
    "X_train = train_df.drop(columns=[target_column])\n",
    "y_train = train_df[target_column]\n",
    "\n",
    "# Split the test set into features (X_test) and target (y_test)\n",
    "X_test = test_df.drop(columns=[target_column])\n",
    "y_test = test_df[target_column]\n",
    "\n",
    "# Display the first few rows of each to verify\n",
    "print(\"Training features (X_train):\")\n",
    "print(X_train.head())\n",
    "print(\"\\nTraining target (y_train):\")\n",
    "print(y_train.head())\n",
    "\n",
    "print(\"\\nTest features (X_test):\")\n",
    "print(X_test.head())\n",
    "print(\"\\nTest target (y_test):\")\n",
    "print(y_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [6 - Model Definition](#Model-Definition)\n",
    "## [6.1 - Define the Logistic Regression Model using Sklearn](#Define-the-Logistic-Regression-Model-using-Sklearn)\n",
    "\n",
    "In this section we will define the Linear Regression Model using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearRegression()\n",
    "ridge_model = Ridge()\n",
    "lasso_model = Lasso()\n",
    "elastic_net_model = ElasticNet()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [6.2 - Finding Best Model](#Finding-Best-Model)\n",
    "\n",
    "In this section we will train different Regression Models on the training set and select the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining four different default models and evaluate their accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Ridge Regression\n",
    "ridge_model = Ridge()\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Lasso Regression\n",
    "lasso_model = Lasso()\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Elastic Net \n",
    "elastic_net_model = ElasticNet()\n",
    "elastic_net_model.fit(X_train, y_train)\n",
    "\n",
    "# Printing accuarcy for the default models\n",
    "print(\"Linear Regression Train Score: \", linear_model.score(X_test, y_test))\n",
    "print(\"Ridge Regression Train Score: \", ridge_model.score(X_test, y_test))  \n",
    "print(\"Lasso Regression Train Score: \", lasso_model.score(X_test, y_test))\n",
    "print(\"Elastic Net Train Score: \", elastic_net_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs hyperparameter tuning using GridSearchCV for Ridge, Lasso, and Elastic Net regression models by exploring different regularization strengths (alpha) and solvers. For Elastic Net, it also tunes the l1_ratio, which balances L1 and L2 regularization. The best model for each type is selected based on cross-validated mean squared error on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression with different solvers\n",
    "ridge_params = {\n",
    "    'alpha': [0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sag', 'saga']\n",
    "}\n",
    "ridge_grid = GridSearchCV(Ridge(), ridge_params, cv=5, scoring='neg_mean_squared_error')\n",
    "ridge_grid.fit(X_train, y_train)\n",
    "best_ridge = ridge_grid.best_estimator_\n",
    "\n",
    "# Lasso Regression with different solvers \n",
    "lasso_params = {\n",
    "    'alpha': [0.01, 0.1, 1, 10, 100],\n",
    "}\n",
    "lasso_grid = GridSearchCV(Lasso(), lasso_params, cv=5, scoring='neg_mean_squared_error')\n",
    "lasso_grid.fit(X_train, y_train)\n",
    "best_lasso = lasso_grid.best_estimator_\n",
    "\n",
    "# Elastic Net with different solvers\n",
    "elastic_net_params = {\n",
    "    'alpha': [0.01, 0.1, 1, 10, 100],\n",
    "    'l1_ratio': [0.1, 0.5, 0.7, 1.0],  # l1_ratio=1 corresponds to Lasso\n",
    "}\n",
    "elastic_net_grid = GridSearchCV(ElasticNet(), elastic_net_params, cv=5, scoring='neg_mean_squared_error')\n",
    "elastic_net_grid.fit(X_train, y_train)\n",
    "best_elastic_net = elastic_net_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This code makes predictions on the test set using Linear Regression, Ridge, Lasso, and Elastic Net models, calculates their Mean Squared Error (MSE), and identifies the best-performing model based on the lowest MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "linear_pred = linear_model.predict(X_test)\n",
    "ridge_pred = best_ridge.predict(X_test)\n",
    "lasso_pred = best_lasso.predict(X_test)\n",
    "elastic_net_pred = best_elastic_net.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "linear_mse = mean_squared_error(y_test, linear_pred)\n",
    "ridge_mse = mean_squared_error(y_test, ridge_pred)\n",
    "lasso_mse = mean_squared_error(y_test, lasso_pred)\n",
    "elastic_net_mse = mean_squared_error(y_test, elastic_net_pred)\n",
    "\n",
    "# Print the MSE results\n",
    "print(f\"Linear Regression MSE: {linear_mse}\")\n",
    "print(f\"Ridge Regression MSE: {ridge_mse} with alpha={best_ridge.alpha}, solver={best_ridge.solver}\")\n",
    "print(f\"Lasso Regression MSE: {lasso_mse} with alpha={best_lasso.alpha}\")\n",
    "print(f\"Elastic Net MSE: {elastic_net_mse} with alpha={best_elastic_net.alpha}, l1_ratio={best_elastic_net.l1_ratio}\")\n",
    "\n",
    "# Determine the best model\n",
    "best_model_name = min(\n",
    "    [('Linear Regression', linear_mse),\n",
    "     ('Ridge Regression', ridge_mse),\n",
    "     ('Lasso Regression', lasso_mse),\n",
    "     ('Elastic Net', elastic_net_mse)],\n",
    "    key=lambda x: x[1]\n",
    ")[0]\n",
    "\n",
    "print(f\"The best model based on the test set is: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model\n",
    "best_ridge.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [6.3 - Fine-Tuning Model](#Fine-Tuning-Model)\n",
    "\n",
    "\n",
    "This step involves fine-tuning the Ridge Regression model by exploring a finer range of alpha values around the initially successful value of 1. By using GridSearchCV with cross-validation on the training set, we systematically search for the optimal alpha that minimizes the mean squared error, potentially improving the model's performance further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a finer grid for alpha\n",
    "ridge_params = {\n",
    "    'alpha': [0.1, 0.5, 1, 2, 5, 10],\n",
    "    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "ridge_grid = GridSearchCV(Ridge(), ridge_params, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the model on the training data\n",
    "ridge_grid.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_ridge_fine_tuned = ridge_grid.best_estimator_\n",
    "\n",
    "# Evaluate the fine-tuned model on the test set\n",
    "ridge_fine_tuned_pred = best_ridge_fine_tuned.predict(X_test)\n",
    "ridge_fine_tuned_mse = mean_squared_error(y_test, ridge_fine_tuned_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Fine-tuned Ridge Regression MSE: {ridge_fine_tuned_mse}\")\n",
    "print(f\"Best alpha after fine-tuning: {best_ridge_fine_tuned.alpha}\")\n",
    "print(f\"Best solver after fine-tuning: {best_ridge_fine_tuned.solver}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_score = best_ridge_fine_tuned.score(X_test, y_test)\n",
    "print(f\"Fine-tuned Ridge Regression Score: {fine_tuned_score}\")\n",
    "normal_score = best_ridge.score(X_test, y_test)\n",
    "print(f\"Normal Ridge Regression Score: {normal_score}\")\n",
    "\n",
    "print(\"Better Model: \", \"Fine-tuned Ridge Regression\" if fine_tuned_score > normal_score else \"Normal Ridge Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the fine-tuned Ridge Regression model with the default Ridge Regression model, we observe that fine-tuning does not lead to any significant improvement in the model's predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = best_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [7 - Model Evaluation](#Model-Evaluation)\n",
    "\n",
    "In this section, we evaluate the performance of the Ridge Regression model using the test dataset. We begin by calculating key metrics such as Mean Squared Error (MSE) and the R² score to quantify the model's predictive accuracy. Additionally, we visualize the model's predictions by plotting the actual vs. predicted values to assess how closely the predictions match the real outcomes. We also include a residual plot to examine the distribution of errors, helping us to identify any patterns or issues that may suggest further improvements are needed. This comprehensive evaluation provides insights into the effectiveness of the Ridge Regression model and guides potential fine-tuning or adjustments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Make predictions using the Ridge Regression model\n",
    "ridge_pred = best_ridge_fine_tuned.predict(X_test)\n",
    "\n",
    "# Step 2: Calculate the Mean Squared Error (MSE)\n",
    "ridge_mse = mean_squared_error(y_test, ridge_pred)\n",
    "\n",
    "# Step 3: Calculate the R² Score\n",
    "ridge_r2 = r2_score(y_test, ridge_pred)\n",
    "\n",
    "# Step 4: Print the evaluation metrics\n",
    "print(f\"Ridge Regression Model Evaluation:\")\n",
    "print(f\"Mean Squared Error (MSE): {ridge_mse}\")\n",
    "print(f\"R² Score: {ridge_r2}\")\n",
    "\n",
    "# Step 5: Plotting Actual vs Predicted Values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, ridge_pred, color='blue', alpha=0.6)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')\n",
    "plt.title('Ridge Regression: Actual vs Predicted Values')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 6: Plotting Residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "residuals = y_test - ridge_pred\n",
    "plt.scatter(ridge_pred, residuals, color='purple', alpha=0.6)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.title('Residuals of Ridge Regression Model')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [8 - Conclusion](#Conclusion)\n",
    "\n",
    "In this project, we aimed to predict house prices using various regression models, with a primary focus on Ridge Regression. The workflow included data preprocessing, model training, fine-tuning, and evaluation using different metrics and visualizations.\n",
    "\n",
    "### Key Takeaways:\n",
    "1. **Data Preprocessing**:\n",
    "    - The data underwent necessary preprocessing steps such as handling missing values, encoding categorical variables, and feature scaling. These steps are critical in ensuring the model's accuracy and reliability.\n",
    "    - It was observed that the dataset had outliers and some non-linear relationships that might have influenced the model’s performance. While Ridge Regression managed to handle these to some extent, the presence of such data points calls for further investigation or more sophisticated methods like outlier detection and non-linear transformation.\n",
    "\n",
    "2. **Model Training and Fine-Tuning**:\n",
    "    - Several models were trained, including Linear Regression, Ridge, Lasso, and Elastic Net. Ridge Regression was identified as the best-performing model based on the initial Mean Squared Error (MSE) on the test set.\n",
    "    - Fine-tuning was performed on Ridge Regression, primarily adjusting the `alpha` parameter to achieve a balance between model complexity and predictive power. However, it was found that the fine-tuning did not significantly improve the model’s performance, indicating that the default settings were already near-optimal for this dataset.\n",
    "\n",
    "3. **Model Evaluation**:\n",
    "    - The evaluation metrics (MSE and R² score) and the accompanying visualizations (Actual vs. Predicted Values, Residual Plots) provided insights into the model’s performance.\n",
    "    - While the Ridge Regression model performed reasonably well, especially for lower range values, it showed weaknesses in predicting higher range values, as evidenced by the scattered residuals and points far from the perfect prediction line.\n",
    "    - The residual plot suggested that while the model performed adequately for most data points, there were areas (particularly for higher predicted values) where the model's predictions deviated significantly from actual values.\n",
    "\n",
    "### Conclusion:\n",
    "Overall, the Ridge Regression model provided a solid starting point for predicting house prices, particularly in managing overfitting through regularization. However, the limitations observed in the model's performance indicate that further refinement could be beneficial. These could include exploring non-linear models, conducting more in-depth feature engineering, and possibly incorporating advanced techniques such as ensemble methods or neural networks to better capture the complexities of the data.\n",
    "\n",
    "### Future Work:\n",
    "- **Explore Non-Linear Models**: Given the patterns observed in the residuals, non-linear models like Random Forest, Gradient Boosting, or even Support Vector Machines could provide better performance.\n",
    "- **Feature Engineering**: Further exploration of feature interactions, polynomial features, or other transformations could help the model better capture the relationship between features and the target variable.\n",
    "- **Outlier Detection**: Implementing outlier detection and removal techniques might help in improving the model’s robustness and accuracy.\n",
    "- **Ensemble Methods**: Combining the strengths of multiple models through ensemble techniques like bagging, boosting, or stacking could further improve predictive performance.\n",
    "\n",
    "This project has laid a strong foundation for predicting house prices, and with additional refinements, the models could become even more accurate and reliable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
